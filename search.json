[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "learn/math-appendix.html",
    "href": "learn/math-appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Appendix\n\n\n\n\n\n\nvec() rules\n\n\n\n\n\\(\\text{vec}(AXB) = (B' \\otimes A)\\text{vec}(X)\\)"
  },
  {
    "objectID": "learn/models/index.html",
    "href": "learn/models/index.html",
    "title": "Models",
    "section": "",
    "text": "Local Projections\n\n\nIf SVARs are like iterative forecasting, then LPs are like direct forecasting.\n\n\n\n\n\n\n\n\n\n\nStructural Vector Autoregressions\n\n\nLike VAR models but causality plays a role.\n\n\n\n\n\n\n\n\n\n\nVector Autoregressions\n\n\nVector Autoregressions (VAR) are probably the most commonly used method to model dynamics between multiple variables.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "learn/models/var/simulation.html",
    "href": "learn/models/var/simulation.html",
    "title": "How do we simulate VAR(p) models?",
    "section": "",
    "text": "Since the VAR model is a data generating process, we should be able to simulate from it. And that is indeed what we will do in this section. Why do we want to simulate from the model? Simply because being able to simulate from a model allows us to explore the model, test theories, and gain insights that are otherwise only gained using maths – which is oftentimes more difficult.\n\n\nWe start simple and ask how we can simulate a VAR(1) model. Remember that VAR(1) was given by \\[\n\\mathbf{y}_{t} = \\mathbf{c}+ \\mathbf{B}_1\\mathbf{y}_{t-1} + \\mathbf{u}_t.\n\\] So what do we need to be able to simulate this model? First, we need to choose how many variables are simultaneously modelled, i.e. we need to decide on the dimension of \\(\\mathbf{y}_t\\) and for how many periods we want to simulate the model. Let’s call these \\(K\\) and \\(T\\) respectively.\n\nK = 3  # Simulating 3 variables\nT = 250  # for 250 periods (think of quarters)\n\nSecond, we need values for all the coefficient vectors and matrices. These are \\(\\mathbf{c}\\) and \\(\\mathbf{B}_1\\). Let’s define these – we set \\(\\mathbf{c}=\\mathbf{0}\\) for simplicity.\n\nB1 = 0.5 * randn(K, K)\nc = zeros(K)\n\nThird, we need to make some assumption on the innovations (the error terms) \\(\\mathbf{u}_t\\).1 A very common assumption is that these are normally distributed. For simplicity, we will stick to standard normals. It’s good practice to draw all of the error terms at the same time.\n1 I will use innovation and error interchangeably to describe \\(\\mathbf{u}_t\\).\nU = randn(T, K)  # each row corresponds to one u_t\n\nThat’s it. We are ready to simulate the model. Or are we? Let’s check how we could get the first period’s value. For the first period we have \\[\n\\mathbf{y}_1 = \\mathbf{c}+ \\mathbf{B}_1\\mathbf{y}_0 + \\mathbf{u}_1.\n\\] What is \\(\\mathbf{y}_0\\). That’s our missing piece. Because we model a dynamic system, and dynamic systems are (at least initially) dependent on a starting value, we need to define a starting value, a value for \\(\\mathbf{y}_0\\). For now, let’s simply choose zero as a starting point.\n\ny0 = zeros(K)\n\nSo now we are ready; we can simulate the model. Because it’s a dynamic system, we simply loop through all the periods we want to simulate.\n\nY = zeros(T+1, K)\nY[1, :] = y0\nfor i=2:(T+1)\n  Y[i, :] = c + B1*Y[i-1, :] + U[i-1, :]\nend\n\nNote that in the code above the indexing is a bit strange. That’s all because we put the initial value in the Y matrix as the first element.\nThat’s it. We just simulated a VAR(1).\n\n\n\ndgp = VAR(B1, diagm(ones(K)))\nmodel = KeynesVAR.simulate(dgp, T, nothing)\n\n\n\n\n\n\nK = 3\np = 4\nN = 250\n\n\n\n\nSigma = diagm(ones(K))\nB = reduce(hcat, 0.5^i * randn(K, K) for i=1:p)\n\n\nusing KeynesUtils\nC = KeynesUtils.make_companion_matrix(B)\nmaximum(abs, eigvals(C))\n\n\ndgp = VAR(B, Sigma)\nmodel = simulate(dgp, T, nothing)\n\n\ntrend_exponents = [0]  # only a constant\nB2 = hcat(randn(K, 1), B)\ndgp = VAR(B2, Sigma)\nmodel = simulate(dgp, T, trend_exponents)\n\n\ntrend_exponents = [0, 1]  # constant and linear trend\nB3 = hcat(randn(K, 2), B)\ndgp = VAR(B3, Sigma)\nmodel = simulate(dgp, T, trend_exponents)\n\n\n# using non Gaussian errors\n# i.e. using Uniform errors\nerrors = rand(K, T+p)\nmodel = simulate!(dgp, errors, trend_exponents)",
    "crumbs": [
      "Vector Autoregressions",
      "Simulation"
    ]
  },
  {
    "objectID": "learn/models/var/simulation.html#simulating-a-var1-model",
    "href": "learn/models/var/simulation.html#simulating-a-var1-model",
    "title": "How do we simulate VAR(p) models?",
    "section": "",
    "text": "We start simple and ask how we can simulate a VAR(1) model. Remember that VAR(1) was given by \\[\n\\mathbf{y}_{t} = \\mathbf{c}+ \\mathbf{B}_1\\mathbf{y}_{t-1} + \\mathbf{u}_t.\n\\] So what do we need to be able to simulate this model? First, we need to choose how many variables are simultaneously modelled, i.e. we need to decide on the dimension of \\(\\mathbf{y}_t\\) and for how many periods we want to simulate the model. Let’s call these \\(K\\) and \\(T\\) respectively.\n\nK = 3  # Simulating 3 variables\nT = 250  # for 250 periods (think of quarters)\n\nSecond, we need values for all the coefficient vectors and matrices. These are \\(\\mathbf{c}\\) and \\(\\mathbf{B}_1\\). Let’s define these – we set \\(\\mathbf{c}=\\mathbf{0}\\) for simplicity.\n\nB1 = 0.5 * randn(K, K)\nc = zeros(K)\n\nThird, we need to make some assumption on the innovations (the error terms) \\(\\mathbf{u}_t\\).1 A very common assumption is that these are normally distributed. For simplicity, we will stick to standard normals. It’s good practice to draw all of the error terms at the same time.\n1 I will use innovation and error interchangeably to describe \\(\\mathbf{u}_t\\).\nU = randn(T, K)  # each row corresponds to one u_t\n\nThat’s it. We are ready to simulate the model. Or are we? Let’s check how we could get the first period’s value. For the first period we have \\[\n\\mathbf{y}_1 = \\mathbf{c}+ \\mathbf{B}_1\\mathbf{y}_0 + \\mathbf{u}_1.\n\\] What is \\(\\mathbf{y}_0\\). That’s our missing piece. Because we model a dynamic system, and dynamic systems are (at least initially) dependent on a starting value, we need to define a starting value, a value for \\(\\mathbf{y}_0\\). For now, let’s simply choose zero as a starting point.\n\ny0 = zeros(K)\n\nSo now we are ready; we can simulate the model. Because it’s a dynamic system, we simply loop through all the periods we want to simulate.\n\nY = zeros(T+1, K)\nY[1, :] = y0\nfor i=2:(T+1)\n  Y[i, :] = c + B1*Y[i-1, :] + U[i-1, :]\nend\n\nNote that in the code above the indexing is a bit strange. That’s all because we put the initial value in the Y matrix as the first element.\nThat’s it. We just simulated a VAR(1).\n\n\n\ndgp = VAR(B1, diagm(ones(K)))\nmodel = KeynesVAR.simulate(dgp, T, nothing)",
    "crumbs": [
      "Vector Autoregressions",
      "Simulation"
    ]
  },
  {
    "objectID": "learn/models/var/simulation.html#simulating-a-varp",
    "href": "learn/models/var/simulation.html#simulating-a-varp",
    "title": "How do we simulate VAR(p) models?",
    "section": "",
    "text": "K = 3\np = 4\nN = 250\n\n\n\n\nSigma = diagm(ones(K))\nB = reduce(hcat, 0.5^i * randn(K, K) for i=1:p)\n\n\nusing KeynesUtils\nC = KeynesUtils.make_companion_matrix(B)\nmaximum(abs, eigvals(C))\n\n\ndgp = VAR(B, Sigma)\nmodel = simulate(dgp, T, nothing)\n\n\ntrend_exponents = [0]  # only a constant\nB2 = hcat(randn(K, 1), B)\ndgp = VAR(B2, Sigma)\nmodel = simulate(dgp, T, trend_exponents)\n\n\ntrend_exponents = [0, 1]  # constant and linear trend\nB3 = hcat(randn(K, 2), B)\ndgp = VAR(B3, Sigma)\nmodel = simulate(dgp, T, trend_exponents)\n\n\n# using non Gaussian errors\n# i.e. using Uniform errors\nerrors = rand(K, T+p)\nmodel = simulate!(dgp, errors, trend_exponents)",
    "crumbs": [
      "Vector Autoregressions",
      "Simulation"
    ]
  },
  {
    "objectID": "documentation/KeynesSVAR/index.html",
    "href": "documentation/KeynesSVAR/index.html",
    "title": "This is just a placeholder",
    "section": "",
    "text": "This is just a placeholder"
  },
  {
    "objectID": "documentation/KeynesAPI/fitting.html",
    "href": "documentation/KeynesAPI/fitting.html",
    "title": "Fitting",
    "section": "",
    "text": "Fitting",
    "crumbs": [
      "Fitting"
    ]
  },
  {
    "objectID": "documentation/KeynesAPI/index.html",
    "href": "documentation/KeynesAPI/index.html",
    "title": "This is just a placeholder",
    "section": "",
    "text": "This is just a placeholder",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "documentation/KeynesVAR/index.html",
    "href": "documentation/KeynesVAR/index.html",
    "title": "This is just a placeholder",
    "section": "",
    "text": "This is just a placeholder"
  },
  {
    "objectID": "documentation/KeynesBootstrap/index.html",
    "href": "documentation/KeynesBootstrap/index.html",
    "title": "This is just a placeholder",
    "section": "",
    "text": "This is just a placeholder"
  },
  {
    "objectID": "documentation/KeynesAPI/models.html",
    "href": "documentation/KeynesAPI/models.html",
    "title": "Models",
    "section": "",
    "text": "Models",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "documentation/KeynesAPI/exceptions.html",
    "href": "documentation/KeynesAPI/exceptions.html",
    "title": "Exceptions",
    "section": "",
    "text": "Exceptions",
    "crumbs": [
      "Exceptions"
    ]
  },
  {
    "objectID": "documentation/KeynesAPI/statistics.html",
    "href": "documentation/KeynesAPI/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Statistics",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "learn/models/svar/index.html",
    "href": "learn/models/svar/index.html",
    "title": "Structural Vector Autoregressions",
    "section": "",
    "text": "This is just a placeholder",
    "crumbs": [
      "Structural Vector Autoregressions"
    ]
  },
  {
    "objectID": "learn/models/var/index.html",
    "href": "learn/models/var/index.html",
    "title": "Vector Autoregressions",
    "section": "",
    "text": "A Vector Autoregression (VAR) is a simple model that links the current data to past observations and an error term. More formally, if we observe a set of variales, a vector of variables, every period and denote this vector with \\(y_t\\), then a VAR(1) model has the form \\[\n\\mathbf{y}_t = \\mathbf{c}+ \\mathbf{B}_1\\mathbf{y}_{t-1} + \\mathbf{u}_t,\n\\] where \\(\\mathbf{c}\\) is some vector of constants, \\(\\mathbf{B}_1\\) the the autoregressive (AR) matrix and \\(\\mathbf{u}_t\\) is some error term with mean \\(\\mathbf{0}\\) and covariance \\(\\boldsymbol{\\Sigma}_u\\). We call this model a VAR(1) model because it relates the current observation \\(\\mathbf{y}_t\\) to a single lagged observation \\(\\mathbf{y}_{t-1}\\).\nThe VAR(1) model can be generalised by extending the number of lags in the model. With \\(p\\) lags in the model, the VAR(p) – note the \\(p\\) in parentheses – becomes \\[\n\\mathbf{y}_t = \\mathbf{c}+ \\sum_{i=1}^p \\mathbf{B}_i \\mathbf{y}_{t-i} + \\mathbf{u}_t,\n\\] where \\(\\mathbf{B}_i\\) is the i-th autoregressive (AR) matrix.\n\n\nAbove are the mathematical exppressions, but what does a VAR model actually do and why do people use it? These are good questions, and VAR models have been used in various ways and thus have also been motivated in various ways. The easiest motivation is to note that a VAR model simply links the current observation to past observation. As such, a VAR model is claiming that there are some dynamics in the system – whether that’s an economy, a business, the oxygen level of a person, etc – and that these dynamics are well described by a linear function that links the current observation to a finite past.\nNaturally, there are many siutations in which both the linear and finite past aspects are doubtful. However, both concerns can somewhat be remedied. First, the finite past limitation can be lifted by working with VAR(\\(\\infty\\)) models. Second, the linearity limitation cannot be lifted but can at least be defended in the following way.\nSuppose the real system actually features non-linear dynamics – however still with a finite past. We could express this mathematically as \\[\n\\mathbf{y}_t = f(\\mathbf{y}_{t-1}, \\ldots, \\mathbf{y}_{t-p}) + \\mathbf{u}_t.\n\\] Also suppose that this non-linear system has some fixed point \\(\\bar{\\mathbf{y}}\\) such that \\[\n\\bar{\\mathbf{y}} = f(\\bar{\\mathbf{y}}, \\ldots, \\bar{\\mathbf{y}}) + \\mathbf{0}.\n\\] Thus, if all previous values are at the fixed point and the innovation (the error term) are zero, then the system will remain at the fixed point. If these two assumptions describe the true system’s dynamics, then the VAR(p) model can be considered as a linear approximation of the possibly complicated non-linear dynamics. Here, the approximation is taken around the fixed point. Thus, taking a first-order Taylor approximation around the fixed point we get \\[\n\\begin{split}\n\\mathbf{y}_t &= f(\\bar{\\mathbf{y}}, \\ldots, \\bar{\\mathbf{y}}) + \\sum_{i=1}^p\\frac{\\partial f}{\\partial \\mathbf{y}_{t-i}}(\\mathbf{y}_{t-i} - \\bar{\\mathbf{y}}) + \\mathbf{u}_t \\\\\n&= \\underbrace{\\left[f(\\bar{\\mathbf{y}}, \\ldots, \\bar{\\mathbf{y}}) - \\sum_{i=1}^p\\frac{\\partial f}{\\partial \\mathbf{y}_{t-i}}\\bar{\\mathbf{y}}\\right]}_{\\mathbf{c}} + \\sum_{i=1}^p \\underbrace{\\frac{\\partial f}{\\partial \\mathbf{y}_{t-i}}}_{\\mathbf{B}_i}\\mathbf{y}_{t-i} + \\mathbf{u}_t,\n\\end{split}\n\\] which is a VAR(p) model.\n\n\n\nAt times it can be convenient to represent the VAR(p) model in other, yet still equivalent, mathematical forms. Each form has advantages and disadvantages.\n\n\nThe matrix form is best used when deriving computations methods. Data is naturally represented as matrices and thus having a matrix form allows us to link the VAR most naturally to data structures used by computers.\nTo move from the standard representation above to the matrix representation, we first take the transpose \\[\n\\mathbf{y}_t' = \\mathbf{c}' + \\sum_{i=1}^p \\mathbf{y}_{t-i}'\\mathbf{B}_i' + \\mathbf{u}_t'.\n\\]\nWe next summarise this transposed system by introducing \\(\\mathbf{z}_t\\) and \\(\\mathbf{B}_+\\), where the former summarises all the data past data up to a finite horizon, and the latter collects all coefficients in a single matrix. \\[\n\\mathbf{z}_t = \\left[\\mathbf{y}_{t-1}', \\ldots, \\mathbf{y}_{t-p}' \\right], \\quad\n\\mathbf{B}_+ = \\left[\\mathbf{c}, \\mathbf{B}_1, \\ldots, \\mathbf{B}_p \\right]\n\\]\nUsing \\(\\mathbf{z}_t\\) and \\(\\mathbf{B}_+\\), we can express the VAR(p) in the following way \\[\n\\mathbf{y}_t' = \\mathbf{z}_t\\mathbf{B}_+' + \\mathbf{u}_t'.\n\\]\nMoving from this form to the matrix form simply consists of stacking the equations for each time period underneath each other. Note though that the first equation is for \\(t=p+1\\), since no data exists for periods \\(t\\leq 0\\). \\[\n\\mathbf{Y}= \\begin{bmatrix}\\mathbf{y}_{p+1}' \\\\ \\mathbf{y}_{p+2}' \\\\ \\vdots \\\\ \\mathbf{y}_{T-1}' \\\\ \\mathbf{y}_{T}' \\end{bmatrix},\n\\quad\n\\mathbf{U}= \\begin{bmatrix}\\mathbf{u}_{p+1}' \\\\ \\mathbf{u}_{p+2}' \\\\ \\vdots \\\\ \\mathbf{u}_{T-1}' \\\\ \\mathbf{u}_{T}' \\end{bmatrix},\n\\quad\n\\mathbf{Z}= \\begin{bmatrix}\\mathbf{z}_{p+1} \\\\ \\mathbf{z}_{p+1} \\\\ \\vdots \\\\ \\mathbf{z}_{T-1} \\\\ \\mathbf{z}_{T} \\end{bmatrix}\n\\]\nThe final matrix form is then given by the following equation \\[\n\\mathbf{Y}= \\mathbf{Z}\\mathbf{B}_+' + \\mathbf{U}.\n\\] Note that this is simply a matrix equation with unknown coefficients. We may thus attempt to solve for the unknowns by using least squares. However, since matrix differentation can be a bit tricky at times, and vector differentation is much more common, we may prefer thinking about the model in a vectorised rather than matrix representation.\n\n\n\nTo move from the matrix representation to the vectorised representation we apply the \\(\\text{vec}\\) operator.1 \\[\n\\begin{split}\n\\mathbf{y}&= \\text{vec}(\\mathbf{Y}) \\\\\n&= \\text{vec}(\\mathbf{Z}\\mathbf{B}_+') + \\text{vec}(\\mathbf{U}) \\\\\n&= \\text{vec}(\\mathbf{Z}\\mathbf{B}_+'\\mathbf{I}) + \\text{vec}(\\mathbf{U}) \\\\\n&= (\\mathbf{I}\\otimes \\mathbf{Z})\\text{vec}(\\mathbf{B}_+') + \\text{vec}(\\mathbf{U})\n&= \\mathbf{X}\\mathbf{b}+ \\mathbf{u}\n\\end{split}\n\\] Note that the last lines substitute \\(\\mathbf{X}= (\\mathbf{I}\\otimes \\mathbf{Z})\\), \\(\\mathbf{b}=\\text{vec}(\\mathbf{B}_+)\\), and \\(\\mathbf{u}= \\text{vec}(\\mathbf{U})\\) to more clearly show the connection to standard linear regressions. This system can now be solves using ordinary least squares or any other method, including penalised methods.\n1 See the mathematical appendix for details.\n\n\nWhile the matrix form and the vector form are often used to derive computational methods to estimate a VAR(p), the companion form is more often used to analyse the dynamics of a VAR(p). Analysing dynamics of a VAR(1) is much easier than those of a VAR(p), since all we need to keep track of is last period’s value. However, it turns out that the previous statement is not entirely correct, the reason for which is the companion form. The companion form of a VAR(p) is basically a re-interpretation that shows that any VAR(p) can be written as a VAR(1) if we extend out state vector. State vectors do not usually come up in VAR analysis, however, \\(\\mathbf{y}_t\\) is nothing but a vector of states, all of which observed, which evolves over time. The choice of states to include at time \\(t\\) is arbitrary, as long as no future information is included – the basic assumption is that time \\(t\\) state vectors are fully observable at time \\(t\\).\nWe can exploit the state interpretation of \\(\\mathbf{y}_t\\) and include past observations into the state vector. Thus, define \\[\n\\tilde{\\mathbf{y}}_t' = (\\mathbf{y}_t', \\mathbf{y}_{t-1}', \\ldots, \\mathbf{y}_{t-p+2}', \\mathbf{y}_{t-p+1}').\n\\]\nThe initial VAR(p) relationship then implies the following dynamic relationship for \\(\\tilde{\\mathbf{y}}_t\\) \\[\n\\tilde{\\mathbf{y}}_t =\n\\begin{bmatrix}\n\\mathbf{B}_1 & \\mathbf{B}_2 & \\ldots & \\mathbf{B}_{p-1} & \\mathbf{B}_p \\\\\n\\mathbf{I}& \\mathbf{O}& \\ldots & \\mathbf{O}& \\mathbf{O}\\\\\n\\mathbf{O}& \\mathbf{I}& \\ldots & \\mathbf{O}& \\mathbf{O}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\mathbf{O}& \\mathbf{O}& \\ldots & \\mathbf{I}& \\mathbf{O}\n\\end{bmatrix}\n\\tilde{\\mathbf{y}}_{t-1} +\n\\begin{bmatrix}\\mathbf{u}_t \\\\ \\mathbf{0} \\\\ \\vdots \\\\ \\mathbf{0} \\\\ \\mathbf{0} \\end{bmatrix},\n\\] which is a VAR(1) in the state vector \\(\\tilde{\\mathbf{y}}_t\\).\nTo move from the companion form back to the original state vector \\(\\mathbf{y}_t\\), we simply define \\[\n\\mathbf{J}= [\\mathbf{I}, \\mathbf{O}, \\ldots, \\mathbf{O}, \\mathbf{O}]\n\\] and pre-multiply \\(\\tilde{\\mathbf{y}}_t\\) by \\(\\mathbf{J}\\) \\[\n\\mathbf{y}_t = \\mathbf{J}\\tilde{\\mathbf{y}}_t.\n\\]\nThus, if we are able to analyse the dynamics of a VAR(1), then we are also able to analyse the dyanmics of a VAR(p). For that reason, much of the other sections will always first focus on the VAR(1) case and will then exploit the companion matrix to generalise the findings to a VAR(p) case.\n\n\n\n\nKeynesVAR uses the matrix representation internally. As such, a VAR DGP is specified by providing \\(\\mathbf{B}_+\\) and the covariance matrix for the error term \\(\\mathbf{u}_t\\).\n\nusing LinearAlgebra\nusing KeynesVAR\nusing Random\n\nK = 3  # Number of variables in the VAR\np = 4  # Number of lags\n\nB = randn(K, K*p)  # No constant included\nSigma_u = diagm(ones(K))  # Covariance of error terms\ndgp_var = VAR(B, Sigma_u)  # Specification of VAR DGP\n\nB = randn(K, K*p + p)  # Including a constant\nSigma_u = diagm(ones(K))  # Covariance of error terms\ndgp_var = VAR(B, Sigma_u)  # Specification of VAR DGP",
    "crumbs": [
      "Vector Autoregressions"
    ]
  },
  {
    "objectID": "learn/models/var/index.html#so-what-does-a-var-model-do",
    "href": "learn/models/var/index.html#so-what-does-a-var-model-do",
    "title": "Vector Autoregressions",
    "section": "",
    "text": "Above are the mathematical exppressions, but what does a VAR model actually do and why do people use it? These are good questions, and VAR models have been used in various ways and thus have also been motivated in various ways. The easiest motivation is to note that a VAR model simply links the current observation to past observation. As such, a VAR model is claiming that there are some dynamics in the system – whether that’s an economy, a business, the oxygen level of a person, etc – and that these dynamics are well described by a linear function that links the current observation to a finite past.\nNaturally, there are many siutations in which both the linear and finite past aspects are doubtful. However, both concerns can somewhat be remedied. First, the finite past limitation can be lifted by working with VAR(\\(\\infty\\)) models. Second, the linearity limitation cannot be lifted but can at least be defended in the following way.\nSuppose the real system actually features non-linear dynamics – however still with a finite past. We could express this mathematically as \\[\n\\mathbf{y}_t = f(\\mathbf{y}_{t-1}, \\ldots, \\mathbf{y}_{t-p}) + \\mathbf{u}_t.\n\\] Also suppose that this non-linear system has some fixed point \\(\\bar{\\mathbf{y}}\\) such that \\[\n\\bar{\\mathbf{y}} = f(\\bar{\\mathbf{y}}, \\ldots, \\bar{\\mathbf{y}}) + \\mathbf{0}.\n\\] Thus, if all previous values are at the fixed point and the innovation (the error term) are zero, then the system will remain at the fixed point. If these two assumptions describe the true system’s dynamics, then the VAR(p) model can be considered as a linear approximation of the possibly complicated non-linear dynamics. Here, the approximation is taken around the fixed point. Thus, taking a first-order Taylor approximation around the fixed point we get \\[\n\\begin{split}\n\\mathbf{y}_t &= f(\\bar{\\mathbf{y}}, \\ldots, \\bar{\\mathbf{y}}) + \\sum_{i=1}^p\\frac{\\partial f}{\\partial \\mathbf{y}_{t-i}}(\\mathbf{y}_{t-i} - \\bar{\\mathbf{y}}) + \\mathbf{u}_t \\\\\n&= \\underbrace{\\left[f(\\bar{\\mathbf{y}}, \\ldots, \\bar{\\mathbf{y}}) - \\sum_{i=1}^p\\frac{\\partial f}{\\partial \\mathbf{y}_{t-i}}\\bar{\\mathbf{y}}\\right]}_{\\mathbf{c}} + \\sum_{i=1}^p \\underbrace{\\frac{\\partial f}{\\partial \\mathbf{y}_{t-i}}}_{\\mathbf{B}_i}\\mathbf{y}_{t-i} + \\mathbf{u}_t,\n\\end{split}\n\\] which is a VAR(p) model.",
    "crumbs": [
      "Vector Autoregressions"
    ]
  },
  {
    "objectID": "learn/models/var/index.html#other-forms",
    "href": "learn/models/var/index.html#other-forms",
    "title": "Vector Autoregressions",
    "section": "",
    "text": "At times it can be convenient to represent the VAR(p) model in other, yet still equivalent, mathematical forms. Each form has advantages and disadvantages.\n\n\nThe matrix form is best used when deriving computations methods. Data is naturally represented as matrices and thus having a matrix form allows us to link the VAR most naturally to data structures used by computers.\nTo move from the standard representation above to the matrix representation, we first take the transpose \\[\n\\mathbf{y}_t' = \\mathbf{c}' + \\sum_{i=1}^p \\mathbf{y}_{t-i}'\\mathbf{B}_i' + \\mathbf{u}_t'.\n\\]\nWe next summarise this transposed system by introducing \\(\\mathbf{z}_t\\) and \\(\\mathbf{B}_+\\), where the former summarises all the data past data up to a finite horizon, and the latter collects all coefficients in a single matrix. \\[\n\\mathbf{z}_t = \\left[\\mathbf{y}_{t-1}', \\ldots, \\mathbf{y}_{t-p}' \\right], \\quad\n\\mathbf{B}_+ = \\left[\\mathbf{c}, \\mathbf{B}_1, \\ldots, \\mathbf{B}_p \\right]\n\\]\nUsing \\(\\mathbf{z}_t\\) and \\(\\mathbf{B}_+\\), we can express the VAR(p) in the following way \\[\n\\mathbf{y}_t' = \\mathbf{z}_t\\mathbf{B}_+' + \\mathbf{u}_t'.\n\\]\nMoving from this form to the matrix form simply consists of stacking the equations for each time period underneath each other. Note though that the first equation is for \\(t=p+1\\), since no data exists for periods \\(t\\leq 0\\). \\[\n\\mathbf{Y}= \\begin{bmatrix}\\mathbf{y}_{p+1}' \\\\ \\mathbf{y}_{p+2}' \\\\ \\vdots \\\\ \\mathbf{y}_{T-1}' \\\\ \\mathbf{y}_{T}' \\end{bmatrix},\n\\quad\n\\mathbf{U}= \\begin{bmatrix}\\mathbf{u}_{p+1}' \\\\ \\mathbf{u}_{p+2}' \\\\ \\vdots \\\\ \\mathbf{u}_{T-1}' \\\\ \\mathbf{u}_{T}' \\end{bmatrix},\n\\quad\n\\mathbf{Z}= \\begin{bmatrix}\\mathbf{z}_{p+1} \\\\ \\mathbf{z}_{p+1} \\\\ \\vdots \\\\ \\mathbf{z}_{T-1} \\\\ \\mathbf{z}_{T} \\end{bmatrix}\n\\]\nThe final matrix form is then given by the following equation \\[\n\\mathbf{Y}= \\mathbf{Z}\\mathbf{B}_+' + \\mathbf{U}.\n\\] Note that this is simply a matrix equation with unknown coefficients. We may thus attempt to solve for the unknowns by using least squares. However, since matrix differentation can be a bit tricky at times, and vector differentation is much more common, we may prefer thinking about the model in a vectorised rather than matrix representation.\n\n\n\nTo move from the matrix representation to the vectorised representation we apply the \\(\\text{vec}\\) operator.1 \\[\n\\begin{split}\n\\mathbf{y}&= \\text{vec}(\\mathbf{Y}) \\\\\n&= \\text{vec}(\\mathbf{Z}\\mathbf{B}_+') + \\text{vec}(\\mathbf{U}) \\\\\n&= \\text{vec}(\\mathbf{Z}\\mathbf{B}_+'\\mathbf{I}) + \\text{vec}(\\mathbf{U}) \\\\\n&= (\\mathbf{I}\\otimes \\mathbf{Z})\\text{vec}(\\mathbf{B}_+') + \\text{vec}(\\mathbf{U})\n&= \\mathbf{X}\\mathbf{b}+ \\mathbf{u}\n\\end{split}\n\\] Note that the last lines substitute \\(\\mathbf{X}= (\\mathbf{I}\\otimes \\mathbf{Z})\\), \\(\\mathbf{b}=\\text{vec}(\\mathbf{B}_+)\\), and \\(\\mathbf{u}= \\text{vec}(\\mathbf{U})\\) to more clearly show the connection to standard linear regressions. This system can now be solves using ordinary least squares or any other method, including penalised methods.\n1 See the mathematical appendix for details.\n\n\nWhile the matrix form and the vector form are often used to derive computational methods to estimate a VAR(p), the companion form is more often used to analyse the dynamics of a VAR(p). Analysing dynamics of a VAR(1) is much easier than those of a VAR(p), since all we need to keep track of is last period’s value. However, it turns out that the previous statement is not entirely correct, the reason for which is the companion form. The companion form of a VAR(p) is basically a re-interpretation that shows that any VAR(p) can be written as a VAR(1) if we extend out state vector. State vectors do not usually come up in VAR analysis, however, \\(\\mathbf{y}_t\\) is nothing but a vector of states, all of which observed, which evolves over time. The choice of states to include at time \\(t\\) is arbitrary, as long as no future information is included – the basic assumption is that time \\(t\\) state vectors are fully observable at time \\(t\\).\nWe can exploit the state interpretation of \\(\\mathbf{y}_t\\) and include past observations into the state vector. Thus, define \\[\n\\tilde{\\mathbf{y}}_t' = (\\mathbf{y}_t', \\mathbf{y}_{t-1}', \\ldots, \\mathbf{y}_{t-p+2}', \\mathbf{y}_{t-p+1}').\n\\]\nThe initial VAR(p) relationship then implies the following dynamic relationship for \\(\\tilde{\\mathbf{y}}_t\\) \\[\n\\tilde{\\mathbf{y}}_t =\n\\begin{bmatrix}\n\\mathbf{B}_1 & \\mathbf{B}_2 & \\ldots & \\mathbf{B}_{p-1} & \\mathbf{B}_p \\\\\n\\mathbf{I}& \\mathbf{O}& \\ldots & \\mathbf{O}& \\mathbf{O}\\\\\n\\mathbf{O}& \\mathbf{I}& \\ldots & \\mathbf{O}& \\mathbf{O}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\mathbf{O}& \\mathbf{O}& \\ldots & \\mathbf{I}& \\mathbf{O}\n\\end{bmatrix}\n\\tilde{\\mathbf{y}}_{t-1} +\n\\begin{bmatrix}\\mathbf{u}_t \\\\ \\mathbf{0} \\\\ \\vdots \\\\ \\mathbf{0} \\\\ \\mathbf{0} \\end{bmatrix},\n\\] which is a VAR(1) in the state vector \\(\\tilde{\\mathbf{y}}_t\\).\nTo move from the companion form back to the original state vector \\(\\mathbf{y}_t\\), we simply define \\[\n\\mathbf{J}= [\\mathbf{I}, \\mathbf{O}, \\ldots, \\mathbf{O}, \\mathbf{O}]\n\\] and pre-multiply \\(\\tilde{\\mathbf{y}}_t\\) by \\(\\mathbf{J}\\) \\[\n\\mathbf{y}_t = \\mathbf{J}\\tilde{\\mathbf{y}}_t.\n\\]\nThus, if we are able to analyse the dynamics of a VAR(1), then we are also able to analyse the dyanmics of a VAR(p). For that reason, much of the other sections will always first focus on the VAR(1) case and will then exploit the companion matrix to generalise the findings to a VAR(p) case.",
    "crumbs": [
      "Vector Autoregressions"
    ]
  },
  {
    "objectID": "learn/models/var/index.html#keynesvar",
    "href": "learn/models/var/index.html#keynesvar",
    "title": "Vector Autoregressions",
    "section": "",
    "text": "KeynesVAR uses the matrix representation internally. As such, a VAR DGP is specified by providing \\(\\mathbf{B}_+\\) and the covariance matrix for the error term \\(\\mathbf{u}_t\\).\n\nusing LinearAlgebra\nusing KeynesVAR\nusing Random\n\nK = 3  # Number of variables in the VAR\np = 4  # Number of lags\n\nB = randn(K, K*p)  # No constant included\nSigma_u = diagm(ones(K))  # Covariance of error terms\ndgp_var = VAR(B, Sigma_u)  # Specification of VAR DGP\n\nB = randn(K, K*p + p)  # Including a constant\nSigma_u = diagm(ones(K))  # Covariance of error terms\ndgp_var = VAR(B, Sigma_u)  # Specification of VAR DGP",
    "crumbs": [
      "Vector Autoregressions"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KeynesXJulia",
    "section": "",
    "text": "some text comes here that explains what KeynesXJulia is, where I am planning to take it and some more text. some text comes here that explains what KeynesXJulia is, where I am planning to take it and some more text. some text comes here that explains what KeynesXJulia is"
  },
  {
    "objectID": "index.html#models",
    "href": "index.html#models",
    "title": "KeynesXJulia",
    "section": "Models",
    "text": "Models\n\n\n\n\n\n\n\nLocal Projections\n\n\nIf SVARs are like iterative forecasting, then LPs are like direct forecasting.\n\n\n\n\n\n\n\n\n\n\nStructural Vector Autoregressions\n\n\nLike VAR models but causality plays a role.\n\n\n\n\n\n\n\n\n\n\nVector Autoregressions\n\n\nVector Autoregressions (VAR) are probably the most commonly used method to model dynamics between multiple variables.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#statistics",
    "href": "index.html#statistics",
    "title": "KeynesXJulia",
    "section": "Statistics",
    "text": "Statistics\n\n\n\n\n\n\n\nForecast Error Variance Decomposition\n\n\nFind out how much each shock contributes to the variance.\n\n\n\n\n\n\n\n\n\n\nForecasts\n\n\nLearn how to predict the future.\n\n\n\n\n\n\n\n\n\n\nHistorical Decompositions\n\n\nWhich shocks happened in the past and how did they contribute to historical developments?\n\n\n\n\n\n\n\n\n\n\nImpulse Response Functions\n\n\nWhat happens if there is a shock to the system? What is the effect of monetary and fiscal policy?\n\n\n\n\n\n\n\nNo matching items"
  }
]