---
title: Vector Autoregressions
description: Vector Autoregressions (VAR) are probably the most commonly used method to model dynamics between multiple variables. 
engine: julia
---


<!-- BEGIN: LATEX COMMANDS -->
\newcommand{\vec}{\text{vec}}
<!-- END: LATEX COMMANDS -->

{{< include /_utils.qmd >}}

# Vector Autoregressions (VAR)

A Vector Autoregression (VAR) is a simple model that links the current data to past observations and an error term. More formally, if we observe a set of variales, a vector of variables, every period and denote this vector with $y_t$, then a VAR(1) model has the form 
$$
\by_t = \bc + \bB_1\by_{t-1} + \bu_t,
$$
where $\bc$ is some vector of constants, $\bB_1$ the the autoregressive (AR) matrix and $\bu_t$ is some error term with mean $\mb{0}$ and covariance $\bSigma_u$. We call this model a VAR(1) model because it relates the current observation $\by_t$ to a single lagged observation $\by_{t-1}$. 

The VAR(1) model can be generalised by extending the number of lags in the model. With $p$ lags in the model, the VAR(p) -- note the $p$ in parentheses -- becomes
$$
\by_t = \bc + \sum_{i=1}^p \bB_i \by_{t-i} + \bu_t, 
$$
where $\bB_i$ is the i-th autoregressive (AR) matrix. 

## So what does a VAR model do?

Above are the mathematical exppressions, but what does a VAR model actually do and why do people use it? These are good questions, and VAR models have been used in various ways and thus have also been motivated in various ways. The easiest motivation is to note that a VAR model simply links the current observation to past observation. As such, a VAR model is claiming that there are some dynamics in the system -- whether that's an economy, a business, the oxygen level of a person, etc -- and that these dynamics are well described by a linear function that links the current observation to a **finite past**.

Naturally, there are many siutations in which both the **linear** and **finite past** aspects are doubtful. However, both concerns can somewhat be remedied. First, the finite past limitation can be lifted by working with VAR($\infty$) models. Second, the linearity limitation cannot be lifted but can at least be defended in the following way. 

Suppose the real system actually features non-linear dynamics -- however still with a finite past. We could express this mathematically as 
$$
\by_t = f(\by_{t-1}, \ldots, \by_{t-p}) + \bu_t.
$$
Also suppose that this non-linear system has some fixed point $\bar{\by}$ such that 
$$
\bar{\by} = f(\bar{\by}, \ldots, \bar{\by}) + \mathbf{0}.
$$
Thus, if all previous values are at the fixed point and the innovation (the error term) are zero, then the system will remain at the fixed point. If these two assumptions describe the true system's dynamics, then the VAR(p) model can be considered as a linear approximation of the possibly complicated non-linear dynamics. Here, the approximation is taken around the fixed point. Thus, taking a first-order Taylor approximation around the fixed point we get
$$
\begin{split}
\by_t &= f(\bar{\by}, \ldots, \bar{\by}) + \sum_{i=1}^p\frac{\partial f}{\partial \by_{t-i}}(\by_{t-i} - \bar{\by}) + \bu_t \\
&= \underbrace{\left[f(\bar{\by}, \ldots, \bar{\by}) - \sum_{i=1}^p\frac{\partial f}{\partial \by_{t-i}}\bar{\by}\right]}_{\bc} + \sum_{i=1}^p \underbrace{\frac{\partial f}{\partial \by_{t-i}}}_{\bB_i}\by_{t-i} + \bu_t,
\end{split}
$$
which is a VAR(p) model. 

## Other Forms

At times it can be convenient to represent the VAR(p) model in other, yet still equivalent, mathematical forms. Each form has advantages and disadvantages. 

### Matrix Form

The matrix form is best used when deriving computations methods. Data is naturally represented as matrices and thus having a matrix form allows us to link the VAR most naturally to data structures used by computers. 

To move from the standard representation above to the matrix representation, we first take the transpose
$$
\by_t' = \bc' + \sum_{i=1}^p \by_{t-i}'\bB_i' + \bu_t'.
$$

We next summarise this transposed system by introducing $\bz_t$ and $\bB_+$, where the former summarises all the data past data up to a finite horizon, and the latter collects all coefficients in a single matrix. 
$$
\bz_t = \left[\by_{t-1}', \ldots, \by_{t-p}' \right], \quad
\bB_+ = \left[\bc, \bB_1, \ldots, \bB_p \right]
$$

Using $\bz_t$ and $\bB_+$, we can express the VAR(p) in the following way
$$
\by_t' = \bz_t\bB_+' + \bu_t'.
$$

Moving from this form to the matrix form simply consists of stacking the equations for each time period underneath each other. Note though that the first equation is for $t=p+1$, since no data exists for periods $t\leq 0$. 
$$
\bY = \begin{bmatrix}\by_{p+1}' \\ \by_{p+2}' \\ \vdots \\ \by_{T-1}' \\ \by_{T}' \end{bmatrix}, 
\quad
\bU = \begin{bmatrix}\bu_{p+1}' \\ \bu_{p+2}' \\ \vdots \\ \bu_{T-1}' \\ \bu_{T}' \end{bmatrix}, 
\quad
\bZ = \begin{bmatrix}\bz_{p+1} \\ \bz_{p+1} \\ \vdots \\ \bz_{T-1} \\ \bz_{T} \end{bmatrix}
$$

The final matrix form is then given by the following equation 
$$
\bY = \bZ\bB_+' + \bU.
$$
Note that this is simply a matrix equation with unknown coefficients. We may thus attempt to solve for the unknowns by using least squares. However, since matrix differentation can be a bit tricky at times, and vector differentation is much more common, we may prefer thinking about the model in a vectorised rather than matrix representation.

### Vectorised Form

To move from the matrix representation to the vectorised representation we apply the $\vec$ operator.^[See the [mathematical appendix](/learn/math-appendix.qmd) for details.]
$$
\begin{split}
\by &= \vec(\bY) \\
&= \vec(\bZ\bB_+') + \vec(\bU) \\
&= \vec(\bZ\bB_+'\bI) + \vec(\bU) \\
&= (\bI \otimes \bZ)\vec(\bB_+') + \vec(\bU)
&= \bX\bb + \bu
\end{split}
$$
Note that the last lines substitute $\bX = (\bI \otimes \bZ)$, $\bb=\vec(\bB_+)$, and $\bu = \vec(\bU)$ to more clearly show the connection to standard linear regressions. This system can now be solves using ordinary least squares or any other method, including penalised methods. 

### Companion Form

While the matrix form and the vector form are often used to derive computational methods to estimate a VAR(p), the companion form is more often used to analyse the dynamics of a VAR(p). Analysing dynamics of a VAR(1) is much easier than those of a VAR(p), since all we need to keep track of is last period's value. However, it turns out that the previous statement is not entirely correct, the reason for which is the companion form. The companion form of a VAR(p) is basically a re-interpretation that shows that any VAR(p) can be written as a VAR(1) if we extend out state vector. State vectors do not usually come up in VAR analysis, however, $\by_t$ is nothing but a vector of states, all of which observed, which evolves over time. The choice of states to include at time $t$ is arbitrary, as long as no future information is included -- the basic assumption is that time $t$ state vectors are fully observable at time $t$. 

We can exploit the state interpretation of $\by_t$ and include past observations into the state vector. Thus, define
$$
\tilde{\by}_t' = (\by_t', \by_{t-1}', \ldots, \by_{t-p+2}', \by_{t-p+1}').
$$

The initial VAR(p) relationship then implies the following dynamic relationship for $\tilde{\by}_t$
$$
\tilde{\by}_t = 
\begin{bmatrix}
\bB_1 & \bB_2 & \ldots & \bB_{p-1} & \bB_p \\
\bI & \bO & \ldots & \bO & \bO \\
\bO & \bI & \ldots & \bO & \bO \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
\bO & \bO & \ldots & \bI & \bO
\end{bmatrix}
\tilde{\by}_{t-1} + 
\begin{bmatrix}\bu_t \\ \mb{0} \\ \vdots \\ \mb{0} \\ \mb{0} \end{bmatrix},
$$
which is a VAR(1) in the state vector $\tilde{\by}_t$. 

To move from the companion form back to the original state vector $\by_t$, we simply define 
$$
\bJ = [\bI, \bO, \ldots, \bO, \bO]
$$
and pre-multiply $\tilde{\by}_t$ by $\bJ$
$$
\by_t = \bJ\tilde{\by}_t.
$$

Thus, if we are able to analyse the dynamics of a VAR(1), then we are also able to analyse the dyanmics of a VAR(p). For that reason, much of the other sections will always first focus on the VAR(1) case and will then exploit the companion matrix to generalise the findings to a VAR(p) case. 

## KeynesVAR

KeynesVAR uses the matrix representation internally. As such, a VAR DGP is specified by providing $\bB_+$ and the covariance matrix for the error term $\bu_t$. 


```{julia}
#| output: false

using LinearAlgebra
using KeynesVAR
using Random

K = 3  # Number of variables in the VAR
p = 4  # Number of lags

B = randn(K, K*p)  # No constant included
Sigma_u = diagm(ones(K))  # Covariance of error terms
dgp_var = VAR(B, Sigma_u)  # Specification of VAR DGP

B = randn(K, K*p + p)  # Including a constant
Sigma_u = diagm(ones(K))  # Covariance of error terms
dgp_var = VAR(B, Sigma_u)  # Specification of VAR DGP

```
